{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, Conv1D, TimeDistributed, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import backend as K\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "model = load_model('saved_model_Stage0_LSTM/LSTM_epoch_53_Stage0.h5')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'], weighted_metrics=[])\n",
    "\n",
    "predictions_train = np.load('Train_Dataset_For_LSTM_Stage1/predictions_train.npy')\n",
    "labels_train = np.load('Train_Dataset_For_LSTM_Stage1/predictions_labels_train.npy')\n",
    "\n",
    "predictions_val = np.load('Train_Dataset_For_LSTM_Stage1/predictions_val.npy')\n",
    "labels_val = np.load('Train_Dataset_For_LSTM_Stage1/predictions_labels_val.npy')\n",
    "\n",
    "predictions_test = np.load('Train_Dataset_For_LSTM_Stage1/predictions_test.npy')\n",
    "labels_test = np.load('Train_Dataset_For_LSTM_Stage1/predictions_labels_test.npy')\n",
    "\n",
    "data_train = []\n",
    "data_train_labels = []\n",
    "\n",
    "for i in range(1000000):\n",
    "    if i + 35 > len(predictions_train):\n",
    "        break\n",
    "    data_train.append(predictions_train[i:i + 35, :])\n",
    "    data_train_labels.append(labels_train[i:i+35, :])\n",
    "\n",
    "print(np.shape(data_train))\n",
    "print(np.shape(data_train_labels))\n",
    "\n",
    "data_train = np.array(data_train)\n",
    "data_train_labels = np.array(data_train_labels)\n",
    "indices = np.arange(len(data_train))\n",
    "np.random.shuffle(indices)\n",
    "data_train = data_train[indices]\n",
    "data_train_labels = data_train_labels[indices]\n",
    "\n",
    "data_val = []\n",
    "data_val_labels = []\n",
    "\n",
    "for i in range(1000000):\n",
    "    if i + 35 > len(predictions_val):\n",
    "        break\n",
    "    data_val.append(predictions_val[i:i + 35, :])\n",
    "    data_val_labels.append(labels_val[i:i+35, :])\n",
    "\n",
    "print(np.shape(data_val))\n",
    "print(np.shape(data_val_labels))\n",
    "\n",
    "data_test = []\n",
    "data_test_labels = []\n",
    "\n",
    "for i in range(1000000):\n",
    "    if i + 35 > len(predictions_test):\n",
    "        break\n",
    "    data_test.append(predictions_test[i:i + 35, :])\n",
    "    data_test_labels.append(labels_test[i:i+35, :])\n",
    "\n",
    "print(np.shape(data_test))\n",
    "print(np.shape(data_test_labels))\n",
    "\n",
    "def evaluate_data(model, x_data, y_data, batch_size, num_segments=10):\n",
    "    segment_size = len(x_data) // num_segments\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    cce = CategoricalCrossentropy()\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = (i + 1) * segment_size if i < num_segments - 1 else len(x_data)\n",
    "\n",
    "        x_segment = x_data[start_idx:end_idx]\n",
    "        y_segment = y_data[start_idx:end_idx]\n",
    "\n",
    "        preds = model.predict(x_segment, batch_size=batch_size, verbose=0)  # (batch, timesteps, num_classes)\n",
    "        y_last = y_segment[:, -1, :]            # (batch, num_classes)\n",
    "        preds_last = preds[:, -1, :]            # (batch, num_classes)\n",
    "\n",
    "        # Loss\n",
    "        loss = cce(y_last, preds_last).numpy()\n",
    "        total_loss += loss * y_last.shape[0]\n",
    "\n",
    "        # Accuracy\n",
    "        pred_label = np.argmax(preds_last, axis=1)\n",
    "        true_label = np.argmax(y_last, axis=1)\n",
    "        acc = np.sum(pred_label == true_label)\n",
    "        total_acc += acc\n",
    "        total_samples += y_last.shape[0]\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_acc / total_samples\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "# Train the model\n",
    "def AGIprogressBar(count, total,start):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "    duration=time.time()-start\n",
    "    print('\\r[%s] %s%s ...%s sec' % (bar, percents, '%', duration),end=' ')\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "Batch = 2400\n",
    "epochs = 100\n",
    "segment_count = 5\n",
    "st = time.time()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    print(f'EP: {ep + 1}')\n",
    "    segment_size = len(data_train) // segment_count\n",
    "    for seg in range(segment_count):\n",
    "        start_idx = seg * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "\n",
    "        x_segment = data_train[start_idx:end_idx]\n",
    "        y_segment = data_train_labels[start_idx:end_idx]\n",
    "\n",
    "        indices = np.arange(len(x_segment))\n",
    "        np.random.shuffle(indices)\n",
    "        x_segment = data_train[indices]\n",
    "        y_segment = data_train_labels[indices]\n",
    "\n",
    "        for i in range(len(x_segment) // Batch):\n",
    "            AGIprogressBar(i, len(x_segment) // Batch, st)\n",
    "            x_batch = x_segment[i * Batch:(i + 1) * Batch]\n",
    "            y_batch = y_segment[i * Batch:(i + 1) * Batch]\n",
    "            model.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "    train_loss, train_acc = evaluate_data(\n",
    "        model=model,\n",
    "        x_data=data_train,\n",
    "        y_data=data_train_labels,\n",
    "        batch_size=2400,\n",
    "        num_segments=5\n",
    "    )\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    print(' ')\n",
    "    print(f'Epoch {ep + 1} Training Loss = {train_loss}')\n",
    "    print(f'Epoch {ep + 1} Training ACC = {train_acc}')\n",
    "\n",
    "    val_loss, val_acc = evaluate_data(\n",
    "        model=model,\n",
    "        x_data=np.array(data_val),\n",
    "        y_data=np.array(data_val_labels),\n",
    "        batch_size=2400,\n",
    "        num_segments=5\n",
    "    )\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f'Epoch {ep + 1} Validation Loss = {val_loss}')\n",
    "    print(f'Epoch {ep + 1} Validation ACC = {val_acc}')\n",
    "\n",
    "    os.makedirs('saved_model_Stage1_LSTM', exist_ok=True)\n",
    "    model.save(f'saved_model_Stage1_LSTM/LSTM_epoch_{ep + 1}_Stage1.h5')\n",
    "    print(f'Model saved at epoch {ep + 1}')\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    model = load_model(f'saved_model_Stage1_LSTM/LSTM_epoch_{ep + 1}_Stage1.h5')\n",
    "\n",
    "best_epoch_accuracy = val_accuracies.index(max(val_accuracies))\n",
    "best_val_accuracy = val_accuracies[best_epoch_accuracy]\n",
    "print(f\"The best epoch based on validation accuracy is: {best_epoch_accuracy + 1}, with accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses[best_epoch_accuracy], val_accuracies[best_epoch_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('saved_model_Stage1_LSTM/LSTM_epoch_46_Stage1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245211, 35, 128)\n",
      "(245211, 35, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_data = np.load('Train_Dataset_For_LSTM_Stage1\\predictions_test.npy')\n",
    "test_data_label = np.load('Train_Dataset_For_LSTM_Stage1\\predictions_labels_test.npy')\n",
    "\n",
    "data_test = []\n",
    "data_test_labels = []\n",
    "\n",
    "for i in range(1000000000):\n",
    "    if i + 35 > len(test_data):\n",
    "        break\n",
    "    data_test.append(test_data[i:i + 35, :])\n",
    "    data_test_labels.append(test_data_label[i:i+35, :])\n",
    "\n",
    "print(np.shape(data_test))\n",
    "print(np.shape(data_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "test_data = tf.data.Dataset.from_tensor_slices((data_test, data_test_labels)).batch(12240).prefetch(tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "plt.rcParams['font.size'] = 7\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "y_test = np.array(data_test_labels)\n",
    "\n",
    "y_true = np.argmax(y_test[:, -1, :], axis=1)\n",
    "\n",
    "y_pred_prob = model.predict(test_data)\n",
    "\n",
    "y_pred_last = y_pred_prob[:, -1, :]  # Shape: (None, 8)\n",
    "\n",
    "y_pred = np.argmax(y_pred_last, axis=1)\n",
    "\n",
    "cm = (confusion_matrix(y_true, y_pred, normalize='true'))*100\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"Accuracy: {accuracy:.10f}\")\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Gesture class labels\n",
    "classes = ['DoublePat', 'FallDown', 'HorizontalSwipe', 'SlowUp', 'SwipeDown', 'SwipeLeft', 'SwipeRight', 'SwipeUp']\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(3.5, 3.5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='.2f', ax=ax, colorbar=False)\n",
    "for text in disp.text_.ravel():\n",
    "    if text.get_text():\n",
    "        text.set_text(f\"{float(text.get_text()):.2f}%\")\n",
    "plt.xticks(rotation=90, ha=\"right\")\n",
    "plt.title(r\"[Exp-3] CM for LSTM: D$_1$ (Bathroom Environment)\")\n",
    "plt.savefig('[Exp-3] CM for LSTM_Stage1.png', dpi=300, bbox_inches='tight')\n",
    "#plt.subplots_adjust(left=0.2, right=0.3, top=0.9, bottom=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook Layers\n",
    "hook=[]\n",
    "id=[2]\n",
    "for i in range(len(id)):\n",
    "  hook.append(model.layers[id[i]].output)\n",
    "ModelExtract = Model(inputs=model.input, outputs=hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(data_test))\n",
    "\n",
    "data_test = np.array(data_test)\n",
    "data_test_labels = np.array(data_test_labels)\n",
    "x_test_shuffled = data_test[indices]\n",
    "y_test_shuffled = data_test_labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ModelExtract.predict(x_test_shuffled[:20000])\n",
    "labels = np.argmax(y_test_shuffled[:20000][:, -1, :], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "classes = ['DoublePat', 'FallDown', 'HorizontalSwipe', 'SlowUp', 'SwipeDown', 'SwipeLeft', 'SwipeRight', 'SwipeUp']\n",
    "label_to_color = {\n",
    "    'DoublePat': [1, 0, 0, 1],           # Red\n",
    "    'SwipeDown': [0, 0.5, 0, 1],         # Green\n",
    "    'SwipeUp': [0, 0, 1, 1],             # Blue\n",
    "    'SlowUp': [1, 0.65, 0, 1],           # Orange\n",
    "    'HorizontalSwipe': [1, 1, 0, 1],     # Yellow\n",
    "    'SwipeLeft': [0, 0, 0, 1],           # Black\n",
    "    'FallDown': [0.93, 0.51, 0.93, 1],   # Violet\n",
    "    'SwipeRight': [0.5, 0, 0.5, 1],      # Purple\n",
    "}\n",
    "\n",
    "input_data = np.reshape(predictions, [len(predictions), -1])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(input_data)\n",
    "Dim2 = pca.transform(input_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "for cl in range(8):\n",
    "    chos = Dim2[np.argwhere(labels == cl).reshape([-1])]\n",
    "    color = label_to_color[classes[cl]]\n",
    "    ax.scatter(chos[:, 0], chos[:, 1], label=classes[cl], facecolors='none', edgecolors=color, marker='o', s=10)\n",
    "ax.set_title(r\"[Exp-3] LSTM: D$_1$, dense: \" + str(predictions.shape))\n",
    "plt.tight_layout()\n",
    "plt.savefig('[Exp-3] PCA for LSTM_dense.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KKT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
